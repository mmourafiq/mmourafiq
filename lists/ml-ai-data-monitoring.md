---
layout: lists
title: "ML/AI/Data Monitoring"
tags: [ai, machine-learning, mlops, python]
---

 * [Aporia](https://www.aporia.com/): Observability with customized monitoring and explainability for ML models.
 * [Arize](https://github.com/Arize-ai/client_python): An end-to-end ML observability and model monitoring platform.
 * [Arthur](https://www.arthur.ai/): Arthur is a platform for monitoring and explaining machine learning models in production.
 * [DataProfiler](https://github.com/capitalone/DataProfiler): A Python library designed to make data analysis, monitoring and sensitive data detection easy.
 * [Deepchecks](https://github.com/deepchecks/deepchecks): Test Suites for Validating ML Models & Data. Deepchecks is a Python package for comprehensively validating your machine learning models and data with minimal effort.
 * [Efemerai](https://github.com/efemarai/efemarai): Platform for testing and improving ML continuously.
 * [Elementary](https://github.com/elementary-data/elementary/): Open-source data observability for analytics engineers. 
 * [Evidently](https://github.com/evidentlyai/evidently): Interactive reports to analyze ML models during validation or production monitoring.
 * [Fiddler](https://www.fiddler.ai/): Monitor, explain, and analyze your AI in production.
 * [Grai](https://github.com/grai-io/grai-core): Data lineage made simple. Grai makes it easy to understand and test how your data relates across databases, warehouses, APIs and dashboards.
 * [Great Expectations](https://github.com/great-expectations/great_expectations): Helps data teams eliminate pipeline debt, through data testing, documentation, and profiling.
 * [Manifold](https://github.com/uber/manifold): A model-agnostic visual debugging tool for machine learning.
 * [Meerkat](https://github.com/HazyResearch/meerkat/): Creative interactive views of any dataset.
 * [NannyML](https://github.com/NannyML/nannyml): post-deployment data science in python. 
 * [Netron](https://github.com/lutzroeder/netron): Visualizer for neural network, deep learning, and machine learning models.
 * [Pandas Profiling](https://github.com/pandas-profiling/pandas-profiling): Extends the pandas DataFrame with df.profile_report() for quick data analysis.
 * [Pandera](https://github.com/pandera-dev/pandera): A light-weight, flexible, and expressive data validation library for dataframes.
 * [Superwise](https://www.superwise.ai): Fully automated, enterprise-grade model observability in a self-service SaaS platform.
 * [Traceml](https://github.com/polyaxon/traceml):  Engine for ML/Data tracking, visualization, explainability, drift detection, and dashboards for Polyaxon.
 * [Whylogs](https://github.com/whylabs/whylogs): The open source standard for data logging. Enables ML monitoring and observability.
 * [ydata-quality](https://github.com/ydataai/ydata-quality): Data Quality assessment with one line of code.
 * [Yellowbrick](https://github.com/DistrictDataLabs/yellowbrick): Visual analysis and diagnostic tools to facilitate machine learning model selection.
 * [Soda Core](https://github.com/sodadata/soda-core): Data profiling, testing, and monitoring for SQL accessible data.
 * [Uptrain](https://github.com/uptrain-ai/uptrain):  Your open-source LLM experimentation, response validation and monitoring toolkit.

