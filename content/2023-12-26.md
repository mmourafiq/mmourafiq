---
layout: list
---

 - [Fake It â€™Til You Fake It](https://pxlnv.com/blog/fake-it-til-you-fake-it/)
 - [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)
 - [The Annotated S4](https://srush.github.io/annotated-s4/)
 - [Mamba - a replacement for Transformers?](https://www.youtube.com/watch?v=ouF-H35atOY)
 - [Structured State Space Models for Deep Sequence Modeling (Albert Gu, CMU)](https://www.youtube.com/watch?v=OpJMn8T7Z34)
 - [Do we need Attention? - Linear RNNs and State Space Models (SSMs) for NLP](https://www.youtube.com/watch?v=dKJEpOtVgXc)
 - [Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Paper Explained)](https://www.youtube.com/watch?v=9dSkvxS2EB0)
