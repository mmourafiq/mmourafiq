---
layout: list
---

 - [A complete end-to-end pipeline for LLM interpretability with sparse autoencoders (SAEs) using Llama 3.2, written in pure PyTorch and fully reproducible](https://github.com/PaulPauls/llama3_interpretability_sae)
 - [Show HN: Llama 3.2 Interpretability with Sparse Autoencoders (github.com/paulpauls)](https://news.ycombinator.com/item?id=42208383)
 - [Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet](https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html)
 - [Scaling and evaluating sparse autoencoders](https://arxiv.org/abs/2406.04093)
 - [Gemma Scope: Open Sparse Autoencoders Everywhere All At Once on Gemma 2](https://arxiv.org/abs/2408.05147)
 - [An Intuitive Explanation of Sparse Autoencoders for LLM Interpretability](https://adamkarvonen.github.io/machine_learning/2024/06/11/sae-intuitions.html)
 - [pingcap/autoflow is a Graph RAG based and conversational knowledge base tool built with TiDB Serverless Vector Storage. Demo: https://tidb.ai](https://github.com/pingcap/autoflow)
 - [Bayesian Neural Networks](https://www.cs.toronto.edu/~duvenaud/distill_bayes_net/public/)
 - [OK, I can partly explain the LLM chess weirdness now](https://dynomight.net/more-chess/)
 - [Amazon S3 Express One Zone now supports the ability to append data to an object](https://aws.amazon.com/about-aws/whats-new/2024/11/amazon-s3-express-one-zone-append-data-object/)
 - [Elon Musk and Vivek Ramaswamy: The DOGE Plan to Reform Government](https://www.wsj.com/opinion/musk-and-ramaswamy-the-doge-plan-to-reform-government-supreme-court-guidance-end-executive-power-grab-fa51c020?st=QxsGqV&reflink=article_copyURL_share)
